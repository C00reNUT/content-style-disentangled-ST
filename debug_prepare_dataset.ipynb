{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import scipy.misc\n",
    "import utils\n",
    "from collections import Counter\n",
    "import img_augm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_WIKIART = '/export/home/asanakoy/workspace/wikiart/images'\n",
    "\n",
    "\n",
    "ARTISTS_LIST = ['paul-cezanne', 'vincent-van-gogh', 'amedeo-modigliani', 'camille-pissarro', 'pierre-auguste-renoir',\n",
    "                'childe-hassam', 'paul-gauguin', 'alfred-sisley', 'claude-monet', 'berthe-morisot']\n",
    "TECHNIQUES_LIST = ['oil', 'watercolor', 'chalk', 'pastel']\n",
    "\n",
    "\n",
    "class ArtDataset():\n",
    "    def __init__(self, path_to_art_dataset, artists_list):\n",
    "        self.artists_list = artists_list\n",
    "        # Read dataset with paintings and corresponding labels.\n",
    "        dataset = pd.read_csv(filepath_or_buffer=path_to_art_dataset,\n",
    "                                  index_col='image_id',\n",
    "                                  sep='\\t',\n",
    "                                  encoding='utf-8')\n",
    "        self.dataset = dataset[dataset['artist_slug'].isin(artists_list)]\n",
    "        print(\"Art dataset size for:\", artists_list, \"is\", self.dataset.shape[0])\n",
    "\n",
    "    def extract_digits(self, s):\n",
    "        return int(''.join(c for c in s if c.isdigit()))\n",
    "\n",
    "    def get_df_statistics_given_column(self, df_orig, column_name, verbose=True):\n",
    "        if column_name not in df_orig.columns:\n",
    "            print(\"Column not found in dataframe.\")\n",
    "            return 0\n",
    "        df = df_orig.copy()\n",
    "        df.fillna(value='NaN', inplace=True)\n",
    "        a = df[column_name].values\n",
    "        col_vals_counts = Counter(a)\n",
    "        if verbose:\n",
    "            print(col_vals_counts)\n",
    "        return col_vals_counts\n",
    "\n",
    "    def prepare_dataset(self,\n",
    "                        input_df,\n",
    "                        wikiart_dir=PATH_TO_WIKIART,\n",
    "                        to_normalize_date=True,\n",
    "                        verbose=True):\n",
    "        print(\"Prepare given dataset.csv...\")\n",
    "        if 'image_id' not in input_df.columns:\n",
    "            input_df['image_id'] = input_df.index\n",
    "\n",
    "        if verbose: print(\"Input. input_df.shape:\", input_df.shape)\n",
    "        input_df = input_df[['artist_slug', 'technique', 'date', 'image_id']]\n",
    "\n",
    "        if verbose: print(\"Leave only relevant columns. Dataframe.shape:\", input_df.shape)\n",
    "        # Leave only specified artists\n",
    "        input_df = input_df[input_df['artist_slug'].isin(self.artists_list)]\n",
    "        if verbose: print(\"Leave only relevant artists. Dataframe.shape:\", input_df.shape)\n",
    "\n",
    "        # Get rid of images without date\n",
    "        input_df = input_df[~input_df['date'].isnull()]\n",
    "        if verbose: print(\"Get rid of unspecified dates. Dataframe.shape:\", input_df.shape)\n",
    "\n",
    "        # Leave only digits in date\n",
    "        if verbose: input_df['date'] = input_df['date'].apply(self.extract_digits)\n",
    "\n",
    "        # Normalize dates for each artist separately.\n",
    "\n",
    "        def date_normalization(df):\n",
    "            df['date'] = (df['date'] - df['date'].min()) / (df['date'].max() - df['date'].min())\n",
    "            return df\n",
    "        if to_normalize_date:\n",
    "            input_df = pd.concat([date_normalization(input_df[input_df['artist_slug'] == artis_slug])\n",
    "                                  for artis_slug in self.artists_list])\n",
    "\n",
    "        # generate column with path to image\n",
    "        input_df['path'] = input_df['image_id'].apply(lambda x: os.path.join(wikiart_dir, x+'.jpg'))\n",
    "        input_df.drop(['image_id'], axis=1, inplace=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dataset statistics for each artist separately.\")\n",
    "            for artist in self.artists_list:\n",
    "                print(\"%s:\" % artist)\n",
    "                self.get_df_statistics_given_column(\n",
    "                    input_df[input_df['artist_slug'] == artist],\n",
    "                    column_name='technique')\n",
    "        print(\"Art dataset preprocessing completed.\")\n",
    "\n",
    "        return input_df\n",
    "\n",
    "    def get_batch(self, augmentor, batch_size=1):\n",
    "        \"\"\"\n",
    "        Reads data from dataframe data containing path to images in column 'path' and, in case of dataframe,\n",
    "         also containing artist name, technique name, and period of creation for given artist.\n",
    "         In case of content images we have only the 'path' column.\n",
    "        Args:\n",
    "            data: dataframe with columns ['path', 'artist_slug', 'technique', 'period']\n",
    "            augmentor: Augmentor object responsible for augmentation pipeline\n",
    "            batch_size: size of batch\n",
    "        Returns:\n",
    "            dictionary with fields: image, artist_slug, artist_slug_onehot\n",
    "            each containing a batch of corresponding values\n",
    "        \"\"\"\n",
    "    \n",
    "        batch_image = []\n",
    "        batch_artist_slug = []\n",
    "        batch_artist_slug_onehot = []\n",
    "    \n",
    "        for _ in range(batch_size):\n",
    "            row = self.dataset.sample(n=1)\n",
    "            image = scipy.misc.imread(name=row['path'].values[0], mode='RGB')\n",
    "\n",
    "            if max(image.shape) > 1800.:\n",
    "                image = scipy.misc.imresize(image, size=1800./max(image.shape))\n",
    "            if max(image.shape) < 800:\n",
    "                # Resize the smallest side of the image to 800px\n",
    "                alpha = 800. / float(min(image.shape))\n",
    "                if alpha < 4.:\n",
    "                    image = scipy.misc.imresize(image, size=alpha)\n",
    "                    image = np.expand_dims(image, axis=0)\n",
    "                else:\n",
    "                    image = scipy.misc.imresize(image, size=[800, 800])\n",
    "    \n",
    "            batch_image.append(augmentor(utils.enhance_image(image)).astype(np.float32))\n",
    "            batch_artist_slug.append(self.artists_list.index(row['artist_slug'].values[0]))\n",
    "            batch_artist_slug_onehot.append(\n",
    "                utils.get_one_hot_encoded_vector(l=len(self.artists_list),\n",
    "                                                 i=self.artists_list.index(row['artist_slug'].values[0]))\n",
    "            )\n",
    "\n",
    "        # Now return a batch in correct form\n",
    "        batch_image = np.asarray(batch_image)\n",
    "        batch_artist_slug_onehot = np.asarray(batch_artist_slug_onehot)\n",
    "\n",
    "        return {\"image\": batch_image,\n",
    "                \"artist_slug\": batch_artist_slug,\n",
    "                \"artist_slug_onehot\": batch_artist_slug_onehot}\n",
    "\n",
    "    def initialize_batch_worker(self, queue, augmentor, batch_size=1, seed=228):\n",
    "        np.random.seed(seed)\n",
    "        while True:\n",
    "            batch = self.get_batch(augmentor=augmentor, batch_size=batch_size)\n",
    "            queue.put(batch)\n",
    "\n",
    "\n",
    "class PlacesDataset():\n",
    "    categories_names = \\\n",
    "        ['/a/abbey', '/a/arch', '/a/amphitheater', '/a/aqueduct', '/a/arena/rodeo', '/a/athletic_field/outdoor',\n",
    "         '/b/badlands', '/b/balcony/exterior', '/b/bamboo_forest', '/b/barn', '/b/barndoor', '/b/baseball_field',\n",
    "         '/b/basilica', '/b/bayou', '/b/beach', '/b/beach_house', '/b/beer_garden', '/b/boardwalk', '/b/boathouse',\n",
    "         '/b/botanical_garden', '/b/bullring', '/b/butte', '/c/cabin/outdoor', '/c/campsite', '/c/campus',\n",
    "         '/c/canal/natural', '/c/canal/urban', '/c/canyon', '/c/castle', '/c/church/outdoor', '/c/chalet',\n",
    "         '/c/cliff', '/c/coast', '/c/corn_field', '/c/corral', '/c/cottage', '/c/courtyard', '/c/crevasse',\n",
    "         '/d/dam', '/d/desert/vegetation', '/d/desert_road', '/d/doorway/outdoor', '/f/farm', '/f/fairway',\n",
    "         '/f/field/cultivated', '/f/field/wild', '/f/field_road', '/f/fishpond', '/f/florist_shop/indoor',\n",
    "         '/f/forest/broadleaf', '/f/forest_path', '/f/forest_road', '/f/formal_garden', '/g/gazebo/exterior',\n",
    "         '/g/glacier', '/g/golf_course', '/g/greenhouse/indoor', '/g/greenhouse/outdoor', '/g/grotto', '/g/gorge',\n",
    "         '/h/hayfield', '/h/herb_garden', '/h/hot_spring', '/h/house', '/h/hunting_lodge/outdoor', '/i/ice_floe',\n",
    "         '/i/ice_shelf', '/i/iceberg', '/i/inn/outdoor', '/i/islet', '/j/japanese_garden', '/k/kasbah',\n",
    "         '/k/kennel/outdoor', '/l/lagoon', '/l/lake/natural', '/l/lawn', '/l/library/outdoor', '/l/lighthouse',\n",
    "         '/m/mansion', '/m/marsh', '/m/mausoleum', '/m/moat/water', '/m/mosque/outdoor', '/m/mountain',\n",
    "         '/m/mountain_path', '/m/mountain_snowy', '/o/oast_house', '/o/ocean', '/o/orchard', '/p/park',\n",
    "         '/p/pasture', '/p/pavilion', '/p/picnic_area', '/p/pier', '/p/pond', '/r/raft', '/r/railroad_track',\n",
    "         '/r/rainforest', '/r/rice_paddy', '/r/river', '/r/rock_arch', '/r/roof_garden', '/r/rope_bridge',\n",
    "         '/r/ruin', '/s/schoolhouse', '/s/sky', '/s/snowfield', '/s/swamp', '/s/swimming_hole',\n",
    "         '/s/synagogue/outdoor', '/t/temple/asia', '/t/topiary_garden', '/t/tree_farm', '/t/tree_house',\n",
    "         '/u/underwater/ocean_deep', '/u/utility_room', '/v/valley', '/v/vegetable_garden', '/v/viaduct',\n",
    "         '/v/village', '/v/vineyard', '/v/volcano', '/w/waterfall', '/w/watering_hole', '/w/wave',\n",
    "         '/w/wheat_field', '/z/zen_garden', '/a/alcove', '/a/apartment-building/outdoor', '/a/artists_loft',\n",
    "         '/b/building_facade', '/c/cemetery']\n",
    "    categories_names = [x[1:] for x in categories_names]\n",
    "\n",
    "    def __init__(self, path_to_dataset):\n",
    "        paths = []\n",
    "        categories = []\n",
    "\n",
    "        nmbr_skipped = 0\n",
    "        categories_skipped = []\n",
    "        start_time = time.time()\n",
    "        for category_idx, category_name in enumerate(tqdm(self.categories_names)):\n",
    "            #category_name = category_name[1:]\n",
    "            #print(\"Process %d/%d category.\" % (category_idx + 1, len(categories_names)))\n",
    "            if os.path.exists(os.path.join(path_to_dataset, category_name)):\n",
    "                for file_name in tqdm(os.listdir(os.path.join(path_to_dataset, category_name))):\n",
    "                    paths.append(os.path.join(path_to_dataset, category_name, file_name))\n",
    "                    categories.append(category_name)\n",
    "            else:\n",
    "                print(\"Category %s can't be found in path %s. Skip it.\" %\n",
    "                      (category_name, os.path.join(path_to_dataset, category_name)))\n",
    "                nmbr_skipped += 1\n",
    "                categories_skipped.append(category_name)\n",
    "\n",
    "        self.dataset = pd.DataFrame(np.array([paths, categories]).T, columns=['path', 'category'])\n",
    "        print(\"\\n\")\n",
    "        print(\"Finished. Constructed Places2 dataset of %d images.\" % len (self.dataset))\n",
    "        print(\"Time elapsed: %fs. Categories skipped: %d.\" % (time.time() - start_time, nmbr_skipped))\n",
    "        print(\"Following categories are skipped:\", categories_skipped, '\\n' * 1)\n",
    "\n",
    "    def get_batch(self, augmentor, batch_size=1):\n",
    "        \"\"\"\n",
    "        Generate bathes of images with attached labels(place category) in two different formats:\n",
    "        textual and one-hot-encoded.\n",
    "        Args:\n",
    "            augmentor: Augmentor object responsible for augmentation pipeline\n",
    "            batch_size: size of batch we return\n",
    "        Returns:\n",
    "            dictionary with fields: image, label_text, label_onehot\n",
    "            each containing a batch of corresponding values\n",
    "        \"\"\"\n",
    "\n",
    "        batch_image = []\n",
    "        batch_class = []\n",
    "        for _ in range(batch_size):\n",
    "            row = self.dataset.sample(n=1)\n",
    "            image = scipy.misc.imread(name=row['path'].values[0], mode='RGB')\n",
    "            image_class = row['category'].values[0]\n",
    "            image = scipy.misc.imresize(image, size=2.)\n",
    "            image_shape = image.shape\n",
    "\n",
    "            if max(image_shape) > 1800.:\n",
    "                image = scipy.misc.imresize(image, size=1800. / max(image_shape))\n",
    "            if max(image_shape) < 800:\n",
    "                # Resize the smallest side of the image to 800px\n",
    "                alpha = 800. / float(min(image_shape))\n",
    "                if alpha < 4.:\n",
    "                    image = scipy.misc.imresize(image, size=alpha)\n",
    "                    image = np.expand_dims(image, axis=0)\n",
    "                else:\n",
    "                    image = scipy.misc.imresize(image, size=[800, 800])\n",
    "\n",
    "            batch_image.append(augmentor(utils.enhance_image(image)).astype(np.float32))\n",
    "            batch_class.append(image_class)\n",
    "\n",
    "        return {\"image\": np.asarray(batch_image),\n",
    "                \"label_text\": batch_class,\n",
    "                \"label_onehot\": np.array(\n",
    "                    [utils.get_one_hot_encoded_vector(l=len(self.categories_names),\n",
    "                                                      i=self.categories_names.index(x)) for x in batch_class])\n",
    "                }\n",
    "\n",
    "    def initialize_batch_worker(self, queue, augmentor, batch_size = 1, seed = 228):\n",
    "        np.random.seed(seed)\n",
    "        while True:\n",
    "            batch = self.get_batch(augmentor=augmentor, batch_size=batch_size)\n",
    "            queue.put(batch)\n",
    "\n",
    "\n",
    "class CocoDataset():\n",
    "    def __init__(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/132 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 264188.16it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 250337.46it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 256673.64it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 5/132 [00:00<00:03, 39.84it/s].70it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 258196.82it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 233338.38it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category a/abbey can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/a/abbey. Skip it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 258222.25it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 9/132 [00:00<00:03, 37.77it/s].40it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 235415.51it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 130110.00it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 12/132 [00:00<00:03, 32.22it/s]64it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 223196.25it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 203650.49it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 17/132 [00:00<00:03, 35.10it/s]63it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 217051.54it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 227644.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category b/basilica can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/b/basilica. Skip it.\n",
      "Category b/bayou can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/b/bayou. Skip it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 20/132 [00:00<00:03, 33.22it/s]37it/s]\u001b[A\n",
      "  0%|          | 0/4681 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 4681/4681 [00:00<00:00, 222342.81it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 225733.23it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 23/132 [00:00<00:03, 32.11it/s]19it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 228321.09it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 236181.72it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█▉        | 26/132 [00:00<00:03, 30.63it/s]62it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 199801.07it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 197184.15it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 29/132 [00:00<00:03, 30.39it/s]58it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 180602.14it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 181971.78it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 32/132 [00:01<00:03, 29.49it/s]40it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 178788.39it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 220866.76it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 35/132 [00:01<00:03, 29.06it/s]10it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 239108.85it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 240039.37it/s]\u001b[A\n",
      "  0%|          | 0/4457 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 4457/4457 [00:00<00:00, 221753.16it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|██▉       | 39/132 [00:01<00:03, 29.56it/s]60it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 267292.73it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 201897.72it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 32%|███▏      | 42/132 [00:01<00:03, 29.17it/s]45it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 235582.12it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 223603.19it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 241221.09it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 36%|███▌      | 47/132 [00:01<00:02, 30.19it/s]98it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 152616.71it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 170363.04it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category f/fairway can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/f/fairway. Skip it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 175783.68it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 39%|███▊      | 51/132 [00:01<00:02, 29.46it/s]39it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 232753.10it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 263226.52it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 41%|████      | 54/132 [00:01<00:02, 29.46it/s]63it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 243843.54it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 201946.32it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 263726.36it/s]\u001b[A\n",
      "  0%|          | 0/4939 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▍     | 58/132 [00:01<00:02, 29.60it/s]57it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 224467.18it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 263869.04it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 222250.11it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 48%|████▊     | 64/132 [00:02<00:02, 30.92it/s]65it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 233515.05it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 268184.86it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 276075.46it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 68/132 [00:02<00:02, 31.06it/s]10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category g/gorge can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/g/gorge. Skip it.\n",
      "Category h/herb_garden can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/h/herb_garden. Skip it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 184808.55it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 229400.02it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 220254.37it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 55%|█████▍    | 72/132 [00:02<00:01, 31.14it/s]93it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 313321.08it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 213822.73it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 268414.84it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 58%|█████▊    | 76/132 [00:02<00:01, 30.76it/s]29it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 158887.19it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 149223.13it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 237573.01it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 61%|██████    | 80/132 [00:02<00:01, 30.50it/s]83it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 185350.83it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 187723.40it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 199953.47it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 64%|██████▎   | 84/132 [00:02<00:01, 30.39it/s]06it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 237993.60it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 259109.18it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 66%|██████▌   | 87/132 [00:02<00:01, 30.06it/s]88it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 274166.19it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 239499.34it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 68%|██████▊   | 90/132 [00:03<00:01, 29.18it/s]03it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 234719.91it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 258531.03it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 70%|███████   | 93/132 [00:03<00:01, 28.08it/s]78it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 251104.81it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 262524.66it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 73%|███████▎  | 96/132 [00:03<00:01, 27.50it/s]20it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 275708.88it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 74%|███████▍  | 98/132 [00:03<00:01, 27.27it/s]32it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 248521.89it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 76%|███████▌  | 100/132 [00:03<00:01, 26.21it/s]7it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 252909.64it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 266098.89it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 78%|███████▊  | 103/132 [00:03<00:01, 26.29it/s]0it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 194667.41it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 239830.75it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 230170.45it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 81%|████████  | 107/132 [00:04<00:00, 26.55it/s]9it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 245712.01it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 216161.12it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 236248.24it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 84%|████████▍ | 111/132 [00:04<00:00, 26.76it/s]5it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 203301.05it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 194485.07it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 228789.37it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 87%|████████▋ | 115/132 [00:04<00:00, 26.79it/s]8it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 217004.38it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 226276.37it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 239924.04it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 90%|█████████ | 119/132 [00:04<00:00, 26.98it/s]8it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 246512.05it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 274001.41it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 249461.98it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 93%|█████████▎| 123/132 [00:04<00:00, 27.21it/s]7it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 216009.72it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 234830.30it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 231139.52it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 96%|█████████▌| 127/132 [00:04<00:00, 27.38it/s]1it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 199696.43it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 247291.08it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 235315.13it/s]\u001b[A\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 132/132 [00:04<00:00, 27.71it/s]2it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category a/apartment-building/outdoor can't be found in path /export/home/dkotoven/workspace/Places2_dataset/data_large/a/apartment-building/outdoor. Skip it.\n",
      "\n",
      "\n",
      "Finished. Constructed Places2 dataset of 624077 images.\n",
      "Time elapsed: 5.220313s. Categories skipped: 7.\n",
      "Following categories are skipped: ['a/abbey', 'b/basilica', 'b/bayou', 'f/fairway', 'g/gorge', 'h/herb_garden', 'a/apartment-building/outdoor'] \n",
      "\n",
      "Art dataset size for: ['paul-cezanne', 'vincent-van-gogh', 'amedeo-modigliani', 'camille-pissarro', 'pierre-auguste-renoir', 'childe-hassam', 'paul-gauguin', 'alfred-sisley', 'claude-monet', 'berthe-morisot'] is 2498\n"
     ]
    }
   ],
   "source": [
    "content_dataset_places = PlacesDataset(path_to_dataset='/export/home/dkotoven/workspace/Places2_dataset/data_large')\n",
    "art_dataset = ArtDataset(path_to_art_dataset='./datasets/relevant_wikiart_plus2.csv', \n",
    "                         artists_list=ARTISTS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = img_augm.Augmentor(crop_size=[256, 256],\n",
    "                                       vertical_flip_prb=0.,\n",
    "                                       hsv_augm_prb=1.0,\n",
    "                                       hue_augm_shift=0.10,\n",
    "                                       saturation_augm_shift=0.10, saturation_augm_scale=0.10,\n",
    "                                       value_augm_shift=0.10, value_augm_scale=0.10,\n",
    "                                       affine_trnsfm_prb=1.0, affine_trnsfm_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor_empty = img_augm.Augmentor(crop_size=[256, 256],\n",
    "                                    scale_augm_prb=0., scale_augm_range=0.2,\n",
    "                                     rotation_augm_prb=0., rotation_augm_range=0.15,\n",
    "\n",
    "                                     hsv_augm_prb=0., \n",
    "                                     hue_augm_shift=0.05,\n",
    "\n",
    "                                     saturation_augm_shift=0.05, saturation_augm_scale=0.05,\n",
    "                 value_augm_shift=0.05, value_augm_scale=0.05,\n",
    "                 affine_trnsfm_prb=0.0, affine_trnsfm_range=0.05,\n",
    "                 horizontal_flip_prb=0.0,\n",
    "                 vertical_flip_prb=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to extract 10 batches of 8 elements: 9.2197s.\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    batch = art_dataset.get_batch(augmentor=augmentor, batch_size=batch_size)\n",
    "print(\"Time elapsed to extract %d batches of %d elements: %.4fs.\" % (10, batch_size, time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to extract 10 batches of 8 elements with augmentor_empty: 6.5652s.\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    batch = art_dataset.get_batch(augmentor=augmentor_empty, batch_size=batch_size)\n",
    "print(\"Time elapsed to extract %d batches of %d elements with augmentor_empty: %.4fs.\" % (10, batch_size, time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to extract 10 batches of 8 elements without resizing: 8.3197s.\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    batch = art_dataset.get_batch(augmentor=augmentor, batch_size=batch_size)\n",
    "print(\"Time elapsed to extract %d batches of %d elements without resizing: %.4fs.\" % (10, batch_size, time.time() - start_time ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
